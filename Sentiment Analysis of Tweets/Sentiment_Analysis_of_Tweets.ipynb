{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c99365",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4ef23cf5898b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4ef23cf5898b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    conda install -c conda-forge jupyterlab\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge jupyterlab\n",
    "conda install -c anaconda ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys , os , re , csv , codecs , numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Input,LSTM,Embedding,Dropout,Activation,BatchNormalization\n",
    "from keras.layers import Bidirectional,GlobalMaxPool1D,GlobalAvgPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints , optimizers, layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01117a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Gas by my house hit $3.39!!!! I\\u2019m going t...\n",
       "1    Theo Walcott is still shit\\u002c watch Rafa an...\n",
       "2    its not that I\\u2019m a GSP fan\\u002c i just h...\n",
       "3    Iranian general says Israel\\u2019s Iron Dome c...\n",
       "4    Tehran\\u002c Mon Amour: Obama Tried to Establi...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.txt')\n",
    "test = pd.read_csv('test_samples.txt')\n",
    "df = pd.concat([train,pd.get_dummies(train['sentiment'])],axis=1)\n",
    "#df.head()\n",
    "train_data = df['tweet_text']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe79722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @jjuueellzz down in the Atlantic city, ventnor...\n",
       "1    Musical awareness: Great Big Beautiful Tomorro...\n",
       "2    On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...\n",
       "3    Kapan sih lo ngebuktiin,jan ngomong doang Susa...\n",
       "4    Excuse the connectivity of this live stream, f...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['tweet_text']\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f27f84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['neutral' , 'negative' , 'positive']\n",
    "y = df[classes].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3346b304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id      False\n",
       "tweet_text    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any()\n",
    "test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2681e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration  parameters\n",
    "LATENT_DIM_DECODER = 400\n",
    "BATCH_SIZE =128\n",
    "EPOCHS = 20\n",
    "LATENT_DIM = 400\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_SEQUENCE_LEN = 1000\n",
    "MAX_NUM_WORDS = 100000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c82818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jrm/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "#NLTK python library for preprocessing\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "#for tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#for stemming\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "#for removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#importing regex library of python\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "#function for performing all preproccing steps at once\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens]#  if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#make a dataframe of preprocessed text\n",
    "df['cleanText']=train_data.map(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "167bc98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       jjuueellzz down in the atlantic city ventnor m...\n",
       "1       musical awareness great big beautiful tomorrow...\n",
       "2       on radio fm fri oct labour analyst shawn hatti...\n",
       "3       kapan sih lo ngebuktiin jan ngomong doang susa...\n",
       "4       excuse the connectivity of this live stream fr...\n",
       "                              ...                        \n",
       "5393    it s a wednesday girls night out as s band wil...\n",
       "5394    night college course sorted just have to enrol...\n",
       "5395    for the st time in years for your splendiferou...\n",
       "5396    nurses day may nursing the heart beat of the h...\n",
       "5397    we have minutes left until the nd episode of s...\n",
       "Name: clean_text, Length: 5398, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['clean_text']=test['tweet_text'].map(lambda s:preprocess(s))\n",
    "test_final = test['clean_text']\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45aea830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking the sentence into unique words/tokens\n",
    "#expecting max tokens to be 20k\n",
    "train_final = df['cleanText']\n",
    "max_feat=40000\n",
    "#tokenize sentence into list of words\n",
    "tokenizer = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
    "#fiiting the tokenizer on out data\n",
    "tokenizer.fit_on_texts(list(train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9efbc47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        gas by my house hit i um going to chapel hill ...\n",
       "1        theo walcott is still shit uc watch rafa and j...\n",
       "2        its not that i um a gsp fan uc i just hate nic...\n",
       "3        iranian general says israel us iron dome can u...\n",
       "4        tehran uc mon amour obama tried to establish t...\n",
       "                               ...                        \n",
       "21460    the day after newark ill be able to say i met ...\n",
       "21461    fec hold farewell session for seven ministers ...\n",
       "21462    luca di montezemolo who s last day was monday ...\n",
       "21463    coffee is pretty much the answer to all questi...\n",
       "21464    niki lauda just confirmed to sky that alonso w...\n",
       "Name: cleanText, Length: 21465, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dda83045",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
    "#fiiting the tokenizer on out data\n",
    "tokenizer2.fit_on_texts(list(test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3674696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34302 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "#converting text into sequence of numbers to feed in neural network\n",
    "sequence_train = tokenizer.texts_to_sequences(train_final)\n",
    "sequence_test = tokenizer2.texts_to_sequences(test_final)\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f8fe819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#LOADING PRETRAINED WORD VECTORS\n",
    "# store all the pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open('glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ee4d3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "#EMBEDDING MATRIX\n",
    "# prepare embedding matrix of words for embedding layer\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if(i < MAX_NUM_WORDS):\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c860fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n"
     ]
    }
   ],
   "source": [
    "max_len = [len(s) for s in sequence_train]\n",
    "print(max(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "000ac8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling all the sequences to a fixed length\n",
    "#dimension of input to the layer should be constant\n",
    "#scaling each comment sequence to a fixed length to 200\n",
    "#comments smaller than 200 will be padded with zeros to make their length as 200\n",
    "max_len=1000\n",
    "#pad the train and text sequence to be of fixed length (in keras input in lstm should be of fixed length sequnece)\n",
    "x_train=pad_sequences(sequence_train,maxlen=max_len)\n",
    "x_test=pad_sequences(sequence_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "384f83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len,\n",
    "  trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8839fc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_words = [len(words) for words in sequence_train]\n",
    "#distribution of sequence\n",
    "plt.hist(len_words, bins = np.arange(0,400,10))\n",
    "plt.show()\n",
    "# we can see that most of the comments have [0,50]  words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1f4a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "#feeding the output of previous layer to the embedding layer that converts \n",
    "#the sequences into vector representation to detect relevance and context \n",
    "#of a particular word\n",
    "embed_layer =embedding_layer(input)\n",
    "#passing the previous output as input to the BI_LSTM layer\n",
    "LSTM_layer = Bidirectional(LSTM(256, return_sequences=True, name='BI_lstm_layer'))(embed_layer)\n",
    "sec_LSTM_layer = Bidirectional(LSTM(256, return_sequences=True, name='BI2_lstm_layer'))(LSTM_layer)\n",
    "batchnorm = BatchNormalization()(sec_LSTM_layer)\n",
    "#dimension reduction using pooling layer\n",
    "red_dim_layer = GlobalAvgPool1D()(batchnorm)\n",
    "##### adding dropout layer for better generalization\n",
    "#setting value as 0.1 , which means 10$ of nodes will be randomly disabled\n",
    "drop_layer = Dropout(0.55)(red_dim_layer)\n",
    "#densely connected layer\n",
    "dense1 = Dense(128,activation='elu')(drop_layer)\n",
    "batchnorm2 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(128,activation='elu')(batchnorm2)\n",
    "batchnorm3 = BatchNormalization()(dense2)\n",
    "dense3 = Dense(128,activation='elu')(batchnorm3)\n",
    "#adding another dropout layer\n",
    "drop_layer2 = Dropout(0.55)(dense3)\n",
    "#adding the output dense layer with sigmoid activation to get result \n",
    "#3  classes as output\n",
    "output_dense = Dense(3,activation='softmax')(drop_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b03770e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         10290900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 512)         1140736   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000, 512)         2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 13,108,695\n",
      "Trainable params: 13,107,159\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#connecting the inputs and outputs to create a model and compiling the model\n",
    "from keras.optimizers import Adagrad,Adam,RMSprop\n",
    "model = Model(inputs=input , outputs = output_dense)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = RMSprop(lr=0.001),\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e83c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jrm/opt/anaconda3/envs/Python3point7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 17172 samples, validate on 4293 samples\n",
      "Epoch 1/30\n",
      " 1728/17172 [==>...........................] - ETA: 1:44:03 - loss: 1.4282 - accuracy: 0.4427"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-322dc8c83cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Python3point7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Python3point7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Python3point7/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Python3point7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fitting the model \n",
    "batch_size=64\n",
    "epochs = 30\n",
    "model.fit(x_train,y,batch_size=batch_size,epochs = epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a51a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
